{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND2MglGFoSi+L7TZP+/7og",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinumide/ml-scratch/blob/ann/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Artificial Neural Network from Scratch\n",
        "Some specifics\n",
        "* configurable number of layers and number of neurons\n",
        "\n",
        "Architecture\n",
        "\n",
        "* Input layer -> 32 neurons\n",
        "* hidden layer1 -> 10 neurons\n",
        "* output layer -> 2 neurons\n",
        "\n",
        "Implement feed forward\n",
        "Implement back propagation\n",
        "record loss/cost, "
      ],
      "metadata": {
        "id": "QK1CiwtDFxuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "c5RbxozIKmTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import list\n",
        "from typings import list, int, float\n",
        "import random\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UiwlbH27KrL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log Loss Cost Function"
      ],
      "metadata": {
        "id": "2qKUAEh0VYcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost():\n",
        "  pass"
      ],
      "metadata": {
        "id": "F2t6Pi9KVdim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid Activation Function\n"
      ],
      "metadata": {
        "id": "RukKRIv_KKxP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMOR7xIcCdCh"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  return 1/(1+ np.exp(-z))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rectified Linear Unit (ReLU) Activation Function "
      ],
      "metadata": {
        "id": "G4kaw3V8T4PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(z):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  return max(0, z)"
      ],
      "metadata": {
        "id": "9uZLdlLUUKaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Neural Network"
      ],
      "metadata": {
        "id": "wBAopYrFK42I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_nodes: int, \n",
        "                 hidden_layers: int\n",
        "                 hidden_layer_nodes: list, \n",
        "                 output_nodes: int, \n",
        "                 learning_rate: float\n",
        "                 ):\n",
        "      # Set number of nodes in input, hidden and output layers.\n",
        "      self.input_nodes = input_nodes #x\n",
        "      self.hidden_layers = hidden_layers #h\n",
        "      self.hidden_layer_nodes = hidden_layer_nodes\n",
        "      self.output_nodes = output_nodes #y\n",
        "\n",
        "      # Initialize weights for each hidden layer\n",
        "      self.weight_matrix = [] # list to keep weights for each layer\n",
        "      self.bias_matrix = [] # list to keep bias for each layer\n",
        "      prev_node = self.input_nodes\n",
        "      \n",
        "      for no_nodes in self.hidden_layer_nodes:\n",
        "        \n",
        "        weights_input_to_hidden = np.random.normal(0.0 \n",
        "                                      (prev_nodes, no_nodes)). # x * h\n",
        "\n",
        "        bias_input_to_hidden = np.random.normal(1.0 \n",
        "                                      (1, no_nodes)). # 1 * h \n",
        "\n",
        "        self.weight_matrix.append(weights_input_to_hidden)\n",
        "        self.bias_matrix.append(bias_input_to_hidden)\n",
        "\n",
        "        prev_nodes = no_nodes\n",
        "\n",
        "      # Initialize weights for the output layer \n",
        "      \n",
        "      weights_hidden_to_output = np.random.normal(0.0\n",
        "                                      (prev_nodes, self.output_nodes)) # h * y\n",
        "      bias_hidden_to_output = np.random.normal(1.0\n",
        "                                      (1, self.output_nodes)) # 1 * y\n",
        "      self.weight_matrix.append(weights_hidden_to_output)\n",
        "      self.bias_matrix.append(bias_hidden_to_output)\n",
        "\n",
        "      self.l_r = learning_rate\n",
        "      \n",
        "      self.activation_function = lambda x : sigmoid(x)  \n",
        "        \n",
        "                    \n",
        "    def train(self, features, targets):\n",
        "      \"\"\"\n",
        "      Train the network on batch of features and targets. \n",
        "      \n",
        "      features: nD array, each row is one data record, each column \n",
        "      is a feature in a data record\n",
        "      targets: 1D array of target values\n",
        "      \"\"\"\n",
        "      n_records = features.shape[0]\n",
        "      delta_weight_matrix = [] #change in weights for each hidden layer\n",
        "      delta_bias_matrix = [] #change in bias for each hidden layer\n",
        "\n",
        "      for i in range(self.hidden_layers):\n",
        "        delta_weight_matrix.append(np.zeros(self.weight_matrix[i].shape))\n",
        "        delta_bias_matrix.append(np.zeros(self.bias_matrix[i].shape))\n",
        "\n",
        "      # add change in weights for output layer\n",
        "      delta_weight_matrix.append(np.zeros(self.weight_matrix[-1].shape))\n",
        "      delta_bias_matrix.append(np.zeros(self.bias_matrix[-1].shape))\n",
        "\n",
        "\n",
        "      for X, y in zip(features, targets):\n",
        "          \n",
        "          outputs = self.feedForward(X)  \n",
        "          \n",
        "          delta_weight_matrix, delta_bias_matrix = self.backpropagation( \n",
        "                                                            X, y, \n",
        "                                                            outputs,\n",
        "                                                            delta_weight_matrix, \n",
        "                                                            delta_bias_matrix\n",
        "                                                          )\n",
        "      self.update_weights(delta_weight_matrix, delta_bias_matrix, n_records)\n",
        "\n",
        "    def feedForward(self, X) -> (np.array):\n",
        "      \"\"\"\n",
        "      Forward pass through the network\n",
        "\n",
        "      X: input\n",
        "      type: nD array\n",
        "      \n",
        "      returns: final output of the network and the output of each hidden nodes \n",
        "      in each hidden layer\n",
        "      rtype: tuple of nD arrays\n",
        "      \"\"\"\n",
        "      outputs = []\n",
        "      inputs = X\n",
        "      for i in range(self.hidden_layers):\n",
        "        \n",
        "        hidden_inputs = np.dot(inputs, self.weights_matrix[i]) + self.bias_matrix[i]  # 1, h input to an hidden layer\n",
        "\n",
        "        hidden_outputs = sigmoid(hidden_inputs) #1, h output of an hidden layer\n",
        "\n",
        "        outputs.append(hidden_outputs)\n",
        "\n",
        "        inputs = hidden_outputs\n",
        "\n",
        "      final_inputs = np.dot(input, self.weights_matrix[-1]) + self.bias_matrix[-1] #1,1\n",
        "\n",
        "      final_outputs = sigmoid(final_inputs) #1,1\n",
        "\n",
        "      outputs.append(final_outputs)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "    def backpropagation(self, \n",
        "                        X, y,\n",
        "                        outputs,  \n",
        "                        delta_weight_matrix, \n",
        "                        delta_bias_matrix\n",
        "                        ) -> (np.array):\n",
        "        \"\"\"\n",
        "        Implement backpropagation using log loss error function\n",
        "        \n",
        "        X: input features\n",
        "        y: target \n",
        "        outputs: outputs from forward pass\n",
        "        delta_weight_matrix: change in weights in each hidden layer\n",
        "        delta_bias_matrix: change in bias in each hidden layer\n",
        "\n",
        "        returns: the change in the weights and biases\n",
        "        rtype: tuple of nD arrays\n",
        "        \"\"\"\n",
        "        final_outputs = outputs[-1]\n",
        "        ouput_error = final_outputs - y \n",
        "\n",
        "        error_term = output_error # 1,1\n",
        "\n",
        "        for i in range(self.hidden_layers, -1, -1):\n",
        "\n",
        "          delta_weight[i] += error_term * hidden_output[i-1]\n",
        "          delta_bias[i] += error_term\n",
        "\n",
        "          error_term = np.dot(error_term, self.weights_matrix[i])\\\n",
        "                       * outputs[i-1] * (1 - outputs[i-1])\n",
        "        \n",
        "          # hidden_error = np.dot(error, self.weights_matrix[i]) # h, 1\n",
        "        \n",
        "          # hidden_error_term =  hidden_error * outputs[i-1] * (1 - outputs[i-1])\n",
        "        \n",
        "          # delta_weights_i_h += hidden_error_term * X[:, None]\n",
        "        \n",
        "        \n",
        "\n",
        "          #delta_bias_i_h +=  hidden_error_term\n",
        "\n",
        "        \n",
        "        \n",
        "        return delta_weight_matrix, delta_bias_matrix\n",
        "\n",
        "\n",
        "    def update_parameters(self, \n",
        "                      delta_weight_matrix, \n",
        "                      delta_bias_matrix,\n",
        "                      n_records):\n",
        "      \"\"\"\n",
        "      Update weights and bias on gradient descent step\n",
        "      \n",
        "      delta_weight_matrix: change in weights in each hidden layer\n",
        "      delta_bias_matrix: change in bias in each hidden layer\n",
        "      n_records: number of records\n",
        "      \"\"\"\n",
        "      for in range(self.hidden_layers + 1):\n",
        "        self.weightmatrix[i] += self.l_r * delta_weight_matrix[i] / n_records \n",
        "        self.bias_matrix[i] += self.l_r * delta_bias_matrix[i] / n_records\n",
        "        \n",
        "\n",
        "    def run(self, features):\n",
        "      \"\"\" \n",
        "      Run a forward pass through the network with input features \n",
        "      \n",
        "      features: 1D array of feature values\n",
        "      \"\"\"\n",
        "      hidden_inputs = np.dot(X, self.weights_input_to_hidden)\n",
        "      hidden_outputs = sigmoid(hidden_input)\n",
        "      \n",
        "      final_inputs = np.dot(hidden_input, self.weights_hidden_to_output)\n",
        "\n",
        "      final_outputs = sigmoid(output_input)\n",
        "\n",
        "      \n",
        "      return final_outputs"
      ],
      "metadata": {
        "id": "xZ1pfm31IOgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Neural Network"
      ],
      "metadata": {
        "id": "IGoRKpqEKju1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 100\n",
        "learning_rate = 0.1\n",
        "hidden_nodes = 2\n",
        "output_nodes = 1\n",
        "\n",
        "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "losses = {'train':[], 'validation':[]}\n",
        "for ii in range(iterations):\n",
        "    # Go through a random batch of 128 records from the training data set\n",
        "    batch = np.random.choice(train_features.index, size=128)\n",
        "    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']\n",
        "                             \n",
        "    network.train(X, y)\n",
        "    \n",
        "    # Printing out the training progress\n",
        "    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n",
        "    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n",
        "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
        "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
        "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    losses['train'].append(train_loss)\n",
        "    losses['validation'].append(val_loss)"
      ],
      "metadata": {
        "id": "sT5pLZNJIvzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}